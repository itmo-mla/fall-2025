# Лабораторная работа №4 — Principal Component Analysis (PCA)

## Цель работы
Целью данной лабораторной работы является изучение метода главных компонент (PCA), его реализации через сингулярное разложение (SVD), определения эффективной размерности выборки, а также сравнения собственной реализации PCA с эталонной реализацией из библиотеки **scikit-learn**.

Работа выполняется на языке **Python** с использованием библиотек `numpy`, `scipy`, `matplotlib`, `pandas` и `sklearn` в соответствии с требованиями курса.

---

## Используемый датасет
В работе используется **Diabetes dataset** из библиотеки `sklearn.datasets`.

Характеристики датасета:
- число объектов: 442
- число признаков: 10
- все признаки числовые
- датасет широко применяется для задач линейной регрессии и анализа размерности

Данные **не хранятся в репозитории** и загружаются автоматически во время выполнения программы.

---

## Предобработка данных
Перед применением PCA данные проходят стандартную предобработку:

1. **Стандартизация признаков**:
   \[
   x_j \leftarrow \frac{x_j - \mu_j}{\sigma_j}
   \]
   где \(\mu_j\) — среднее значение признака, \(\sigma_j\) — стандартное отклонение.

2. **Центрирование данных** перед применением PCA:
   \[
   X_c = X - \bar{X}
   \]

Стандартизация необходима, поскольку PCA чувствителен к масштабу признаков, а вклад компонент определяется дисперсией данных.

---

## Теоретические основы PCA

### PCA и сингулярное разложение
Для центрированной матрицы данных \(X_c \in \mathbb{R}^{n \times d}\) выполняется сингулярное разложение:

\[
X_c = U S V^T
\]

где:
- \(U\) — матрица левых сингулярных векторов
- \(S\) — диагональная матрица сингулярных значений
- \(V\) — матрица главных направлений (principal components)

Главные компоненты PCA соответствуют строкам матрицы \(V^T\).

### Объяснённая дисперсия
Собственные значения ковариационной матрицы выражаются через сингулярные значения:

\[
\lambda_i = \frac{S_i^2}{n - 1}
\]

Доля объяснённой дисперсии:
\[
\text{EVR}_i = \frac{\lambda_i}{\sum_j \lambda_j}
\]

Кумулятивная объяснённая дисперсия:
\[
\text{CUM}_k = \sum_{i=1}^k \text{EVR}_i
\]

---

## Реализация PCA через SVD

В работе реализован собственный алгоритм PCA без использования готовых реализаций алгоритма.

Основные этапы:
1. Центрирование данных
2. SVD разложение матрицы данных
3. Вычисление explained variance и explained variance ratio
4. Проекция данных в пространство главных компонент
5. Обратное преобразование и оценка ошибки реконструкции

### Ключевые функции реализации

- `pca_via_svd(X)` — вычисление PCA через SVD
- `transform(X, n_components)` — проекция данных
- `inverse_transform(Z)` — восстановление данных
- `reconstruction_mse(X, X_rec)` — ошибка реконструкции

---

## Эффективная размерность выборки

Эффективная размерность определяется как минимальное число компонент \(k\), при котором кумулятивная объяснённая дисперсия превышает заданный порог:

\[
\text{CUM}_k \geq \tau
\]

В данной работе используется порог:
\[
\tau = 0.95
\]

### Результат
Для стандартизированных данных Diabetes:

- **Эффективная размерность: k = 8**
- Исходная размерность: 10

Это означает, что данные можно сократить с 10 до 8 измерений, сохранив не менее 95% информации.

---

## Сравнение с эталонной реализацией (sklearn)

Для проверки корректности реализации используется `sklearn.decomposition.PCA`.

### Методы сравнения
1. **Численное сравнение explained variance ratio**
2. **Сравнение подпространств главных компонент** с помощью матриц проекций:

\[
||P_1 - P_2||_F
\]

Этот критерий инвариантен к смене знака компонент и проверяет совпадение подпространств.

### Результаты сравнения

- Максимальная разница EVR: `0.0`
- Расстояние между подпространствами: `~10^{-16}`

Полученные значения находятся в пределах машинной точности и подтверждают эквивалентность реализаций.

---

## Экспериментальные результаты

В данном разделе приведены численные результаты запуска программы для различных конфигураций, а также описание полученных графиков.

### 1. Запуск на Diabetes dataset (со стандартизацией)

Команда запуска:
```bash
python source/main.py --standardize
```

Полученные результаты:
- Размерность данных: (442, 10)
- Стандартизация признаков: включена
- Эффективная размерность (порог 0.95): **k = 8**
- Максимальное отклонение explained variance ratio от sklearn: **0.0**
- Расстояние между подпространствами (Frobenius norm): **2.87 × 10⁻¹⁶**

Сохранённые графики:
- `outputs/explained_variance_ratio.png`
- `outputs/cumulative_explained_variance.png`
- `outputs/reconstruction_mse_curve.png`

### 2. Запуск на Diabetes dataset (без стандартизации)

Команда запуска:
```bash
python source/main.py
```

Полученные результаты:
- Размерность данных: (442, 10)
- Стандартизация признаков: отключена
- Эффективная размерность (порог 0.95): **k = 8**
- Максимальное отклонение explained variance ratio от sklearn: **0.0**
- Расстояние между подпространствами (Frobenius norm): **3.55 × 10⁻¹⁶**

Сохранённые графики аналогичны предыдущему запуску.

### 3. Запуск на OpenML House Prices (со стандартизацией)

Команда запуска:
```bash
python source/main.py --dataset "openml:house_prices" --standardize
```

Полученные результаты:
- Размерность данных: (1121, 37)
- Стандартизация признаков: включена
- Эффективная размерность (порог 0.95): **k = 27**
- Максимальное отклонение explained variance ratio от sklearn: **0.0**
- Расстояние между подпространствами (Frobenius norm): **9.09 × 10⁻¹⁶**

Сохранённые графики:
- `outputs/explained_variance_ratio.png`
- `outputs/cumulative_explained_variance.png`
- `outputs/reconstruction_mse_curve.png`

---

## Анализ графиков

### 1. Explained Variance Ratio
Показывает вклад каждой главной компоненты:
- первая компонента объясняет ~40% дисперсии
- последующие компоненты дают убывающий вклад

### 2. Кумулятивная объяснённая дисперсия
- рост кумулятивной дисперсии быстрый на первых компонентах
- порог 95% достигается при k = 8

### 3. Reconstruction MSE
- ошибка реконструкции убывает с ростом числа компонент
- при k = 8 ошибка становится близкой к нулю
- при k = 10 достигается точное восстановление

### 4. Сравнение cumulative variance (Own PCA vs sklearn)
- кривые полностью совпадают
- подтверждает корректность реализации

---

## Выводы

В ходе лабораторной работы:
- реализован алгоритм PCA через сингулярное разложение
- определена эффективная размерность выборки
- проведено строгое сравнение с эталонной реализацией
- показано, что собственная реализация полностью эквивалентна sklearn PCA

Метод PCA эффективно уменьшает размерность данных без существенной потери информации и является важным инструментом анализа данных и подготовки признаков для задач машинного обучения.

---

## Используемые библиотеки
- numpy
- scipy
- matplotlib
- pandas
- scikit-learn

---

## Запуск программы

```bash
python source/main.py --standardize
```

Результаты и графики сохраняются в директории `outputs/`.

