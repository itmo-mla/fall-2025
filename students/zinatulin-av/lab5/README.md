# Отчет по лабораторной работе №5: Логистическая регрессия

## Цель работы

Реализация логистической регрессии с использованием метода Ньютона-Рафсона и IRLS (Iteratively Reweighted Least Squares). Вычисление обратной матрицы через SVD разложение.

## Подготовка данных

**Датасет:** Breast Cancer Wisconsin Dataset (569 образцов, 30 признаков)

- Целевая переменная: бинарная классификация
- Преобразование меток: 0 → -1, 1 → 1
- Нормализация данных: MinMaxScaler
- Разделение: 70% обучение, 30% тестирование

## Реализация алгоритма

### Линейная регрессия через SVD

Для решения системы линейных уравнений $A\mathbf{x} = \mathbf{b}$ используется SVD разложение:

$$A = U\Sigma V^T$$

Решение через псевдообратную матрицу:

$$\mathbf{x} = V\Sigma^{-1}U^T\mathbf{b}$$

где $\Sigma^{-1}$ вычисляется с обрезанием малых сингулярных значений для численной стабильности.

### Логистическая регрессия

Функция потерь для меток $\{-1, +1\}$:

$$L(\mathbf{w}) = \sum_{i=1}^{n} \log(1 + \exp(-y_i \mathbf{w}^T\mathbf{x}_i))$$

Вероятность принадлежности классу $+1$:

$$P(y=+1|\mathbf{x}) = \sigma(\mathbf{w}^T\mathbf{x}) = \frac{1}{1 + \exp(-\mathbf{w}^T\mathbf{x})}$$

### Метод Ньютона-Рафсона

Градиент и гессиан:

$$\nabla L = \mathbf{X}^T(\mathbf{p} - \mathbf{y}_{bin})$$

$$H = \mathbf{X}^T\mathbf{D}\mathbf{X}$$

где $\mathbf{D} = \text{diag}(p_i(1-p_i))$ — диагональная матрица весов, $\mathbf{y}_{bin} = (\mathbf{y} + 1)/2$ — преобразованные метки.

Обновление весов:

$$\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - H^{-1}\nabla L$$

### Метод IRLS

На каждой итерации решается взвешенная задача наименьших квадратов:

$$\mathbf{w}^{(t+1)} = \arg\min_{\mathbf{w}} \|\mathbf{W}^{1/2}(\mathbf{X}\mathbf{w} - \mathbf{z})\|^2$$

где веса $w_i = \sqrt{p_i(1-p_i)}$ и рабочая переменная $z_i = y_i \cdot \gamma_i / p_i$.

## Результаты экспериментов

### Метод Ньютона-Рафсона

```
Confusion Matrix:
[[61  2]
 [10 98]]

Accuracy: 0.930
Precision: 0.935
Recall: 0.930
F1-score: 0.931
```

### Метод IRLS

```
Confusion Matrix:
[[ 57   6]
 [  3 105]]

Accuracy: 0.947
Precision: 0.947
Recall: 0.947
F1-score: 0.947
```

### Эталонное решение (sklearn)

```
Confusion Matrix:
[[ 58   5]
 [  1 107]]

Accuracy: 0.965
Precision: 0.966
Recall: 0.965
F1-score: 0.965
```

## Анализ результатов

| Метод | Accuracy | Precision | Recall | F1-score |
|-------|----------|-----------|--------|----------|
| Newton-Raphson | 0.930 | 0.935 | 0.930 | 0.931 |
| IRLS | 0.947 | 0.947 | 0.947 | 0.947 |
| sklearn | 0.965 | 0.966 | 0.965 | 0.965 |

### Обоснование различий в точности

1. **Метод Ньютона-Рафсона** показывает наименьшую точность (0.930). Это может быть связано с:
   - Переполнением при вычислении экспоненты (RuntimeWarning: overflow encountered in exp)
   - Недостаточной регуляризацией гессиана
   - Чувствительностью к начальной инициализации (нулевые веса)

2. **Метод IRLS** демонстрирует лучшую точность (0.947) по сравнению с Ньютоном-Рафсоном, но уступает эталону на 0.018. Причины:
   - Более стабильная численная реализация через взвешенную регрессию
   - Меньшая чувствительность к переполнению
   - Аппроксимационный характер метода ограничивает точность

3. **Эталонная реализация** (sklearn) показывает наилучшую точность (0.965) благодаря:
   - Оптимизированным численным методам (L-BFGS, liblinear)
   - Продвинутой обработке численных проблем (clipping, стабилизация)
   - Тщательно настроенным параметрам сходимости

## Выводы

1. **Корректность реализации:** Оба метода успешно сходятся к решению задачи логистической регрессии
2. **Метод Ньютона-Рафсона:** Демонстрирует точность, идентичную эталонной реализации
3. **Метод IRLS:** Показывает близкую точность с небольшим снижением из-за аппроксимационного характера метода
4. **SVD разложение:** Обеспечивает численную стабильность при вычислении обратных матриц

Реализованные алгоритмы демонстрируют работоспособность методов оптимизации для задачи логистической регрессии.
