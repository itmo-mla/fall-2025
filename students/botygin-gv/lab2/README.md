# Лабораторная работа №2

В рамках лабораторной работы предстоит реализовать алгоритм классификации KNN и подобрать параметр k методом скользящего контроля.

На лекции были рассмотрены следующие алгоритмы:
* алгоритм метрической классификации KNN;
* метод окна Парзена;
* алгоритм отбора эталонов.

## Задание

1. выбрать датасет для классификации, например на [kaggle](https://www.kaggle.com/datasets?tags=13302-Classification);
2. реализовать алгоритм KNN с методом окна Парзена переменной ширины;
   1. в качестве ядра можно использовать гауссово ядро;
3. подобрать параметр k методом скользящего контроля (LOO);
4. обосновать выбор параметров алгоритма, построить графики эмпирического риска для различных k;
5. сравнить с [эталонной](https://scikit-learn.org/stable/) реализацией KNN;
   1. сравнить качество работы алгоритмов;
6. реализовать алгоритм отбора эталонов;
7. подготовить визуализацию результатов работы алгоритма отбора эталонов;
8. сравнить качество работы KNN с и без отбора эталонов; 
9. подготовить небольшой отчет о проделанной работе.

## Отчет

Для экспериментов был использован датасет Wine:  
  - 178 объектов,  
  - 13 признаков,  
  - 3 класса.

### Подбор k методом LOO

Оптимальное значение: k=1

Это может быть обусловлено тем, что исходный датасет хорошо разделим и практически не содержит шума или выбросов. В экспериментах с искусственно зашумлёнными данными оптимальное значение 
k обычно лежало в диапазоне от 3 до 5.

График эмпирического риска для различных значений k
![](./source/images/risk.png)

### Результаты

Реализация `ParzenWindowKNN` (k=1): 0.7037<br>
Эталонная реализация `sklearn.neighbors.KNeighborsClassifier`: 0.6852

### Отбор эталонов
Был реализован жадный отбор эталонов по критерию CCV. После применения алгоритма количество объектов в датасете сократилось с 178 до 66

После применения алгоритма точность классификатора возрасла до 0.8148

Визуализация алгоритма отбора эталонов (приведена к двумерному виду методом LinearDiscriminantAnalysis)
![](./source/images/prototype_selection.png)