# Лабораторная работа №4

В рамках лабораторной работы предстоит реализовать PCA и сравнить с эталонной реализацией алгоритма.

## Теоретическая часть
На лекции рассмотрели многомерную линейную регрессию. Показали, как с помощью сингулярного разложения можно решать задачи линейной регрессии с L2 регуляризацией (гребневая регрессия). Рассмотрели методы подбора параметра регуляризации. Разобрали PCA, показали как PCA связано с сингулярным разложением и как его можно использовать для снижения размерности данных.

## Задание
- выбрать датасет для линейной регрессии;
- реализовать PCA через сингулярное разложение;
- определить эффективную размерность выборки;
- показать эквивалентность с эталонной реализацией;
- подготовить отчет.

## Отчет

- Использован встроенный датасет `fetch_california_housing` из sklearn.
- Исходная размерность: **20 640 × 8**.
- Эффективная размерность (95% объяснённой дисперсии): **3**.

### Методы
- Реализован кастомный PCA через сингулярное разложение (SVD)
- Проверена эквивалентность кастомной реализации и эталонной (sklearn) — результаты совпадают.
- Обучены две модели линейной регрессии:
  - на исходных признаках;
  - на признаках, сжатых до эффективной размерности (3 компоненты).

## Результаты

| Модель                     | MSE    | R²     |
|---------------------------|--------|--------|
| Исходные данные           | 0.5559 | 0.5758 |
| PCA (кастомный / sklearn) | 0.6293 | 0.5198 |

## Вывод
- Снижение размерности с 8 до 3 признаков привело к ухудшению качества модели:
- Это ожидаемо: при сохранении 95% дисперсии всё ещё теряется часть информации, полезной для предсказания целевой переменной.
- Кастомная реализация PCA корректна — метрики совпадают с sklearn.
