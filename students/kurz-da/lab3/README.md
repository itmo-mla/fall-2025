# Отчет по лабораторной работе №3  
## Метод опорных векторов (SVM): двойственная постановка задачи и трюк с ядром

---

## 1. Цель работы

Целью лабораторной работы является реализация метода опорных векторов (SVM) в **двойственной постановке**, численное решение задачи оптимизации по множителям Лагранжа, применение **kernel trick** для построения нелинейных классификаторов, визуализация полученных решений и сравнение с эталонной реализацией из библиотеки *scikit-learn*.

---

## 2. Данные и предварительная обработка

Для проведения экспериментов использован датасет **Breast Cancer Wisconsin**:  
https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data

Датасет содержит бинарную классификацию опухолей:
- **M** — злокачественная  
- **B** — доброкачественная  

В ходе подготовки данных:
- целевая переменная приведена к значениям \(\{-1, +1\}\);
- выполнена Z-нормализация признаков;
- данные разделены на обучающую и тестовую выборки;
- для визуализации данные дополнительно спроецированы в двумерное пространство с помощью PCA.

---

## 3. Теория и реализация

### 3.1. Прямая (primal) постановка SVM

В линейной постановке SVM требуется найти гиперплоскость:
\[
\langle w, x \rangle + b = 0
\]

при ограничениях:
\[
y_i (\langle w, x_i \rangle + b) \ge 1 - \xi_i, \quad \xi_i \ge 0
\]

Вектор \(w\) определяет направление разделяющей гиперплоскости, а параметр \(b\) — её смещение.

---

### 3.2. Переход к двойственной задаче

Для решения задачи вводятся множители Лагранжа \(\lambda_i\).  
Используя условия стационарности лагранжиана, исходная задача сводится к **двойственной (dual) задаче**:

\[
\max_{\lambda}
\sum_{i=1}^{n} \lambda_i
-
\frac{1}{2}
\sum_{i,j} \lambda_i \lambda_j y_i y_j
\langle \phi(x_i), \phi(x_j) \rangle
\]

при ограничениях:
\[
\sum_i \lambda_i y_i = 0,
\qquad
0 \le \lambda_i \le C
\]

Ключевое наблюдение состоит в том, что зависимость от признаков входит только через скалярное произведение \(\langle \phi(x_i), \phi(x_j) \rangle\).

---

### 3.3. Трюк с ядром (Kernel Trick)

Вместо явного отображения \(\phi(x)\) используется ядро:
\[
K(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle
\]

В работе реализованы:
- линейное ядро  
  \[
  K(x_i, x_j) = \langle x_i, x_j \rangle
  \]
- RBF-ядро  
  \[
  K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)
  \]

Это позволяет строить как линейные, так и нелинейные классификаторы без явного перехода в пространство признаков \(\phi(x)\).

---

### 3.4. Реализация двойственной задачи по \(\lambda\)

Матрица квадратичного члена двойственной функции записывается в виде:
\[
P_{ij} = y_i y_j K(x_i, x_j)
\]

В коде это реализовано следующим образом:

```python
K = self._kernel_function(X, X)
P = np.outer(y, y) * K
```

Целевая функция для численной минимизации (эквивалент максимизации dual-функции):

```python
def objective(lambdas):
    return 0.5 * np.dot(lambdas, np.dot(P, lambdas)) - np.sum(lambdas)
```

Аналитический градиент используется для ускорения сходимости:

```python
def objective_grad(lambdas):
    return np.dot(P, lambdas) - np.ones(n_samples)
```

---

### 3.5. Ограничения и численное решение

Ограничение
\[
\sum_i \lambda_i y_i = 0
\]
возникает из условия стационарности лагранжиана по смещению \(b\).

Границы
\[
0 \le \lambda_i \le C
\]
соответствуют soft-margin постановке и контролируют вклад каждого объекта в решение.

Численное решение выполняется методом SLSQP:

```python
result = minimize(
    fun=objective,
    x0=initial_lambdas,
    method='SLSQP',
    jac=objective_grad,
    bounds=bounds,
    constraints=constraints
)
```

---

### 3.6. Восстановление классификатора и bias

После нахождения оптимальных \(\lambda_i\) классификатор имеет вид:
\[
f(x) = \sum_i \lambda_i y_i K(x_i, x) + b
\]

Смещение \(b\) вычисляется по опорным векторам, удовлетворяющим условию \(0 < \lambda_i < C\).

---

### 3.7. Опорные векторы

Опорными векторами являются объекты, для которых \(\lambda_i > 0\).  
Именно они определяют положение разделяющей поверхности, тогда как остальные точки не влияют на решение.

---

## 4. Результаты экспериментов

### Линейное ядро

- Найдено опорных векторов: **65**
- Accuracy: **0.9790**
- Accuracy (sklearn): **0.9790**

Визуализация решения:

![SVM Linear](images/svm_svm_(линейное_ядро).png)

---

### RBF-ядро (нелинейный классификатор)

- Найдено опорных векторов: **141**
- Accuracy: **0.9720**
- Accuracy (sklearn): **0.9720**

Визуализация решения:

![SVM RBF](images/svm_svm_(rbf_ядро).png)

---

## 5. Сравнение с эталонной реализацией

Для сравнения использована реализация `SVC` из библиотеки *scikit-learn* с теми же параметрами ядра и регуляризации.

Для обоих типов ядер собственная реализация показала **полное совпадение Accuracy** с эталонной, что подтверждает корректность реализации двойственной постановки SVM и kernel trick.

---

## 6. Выводы

В ходе лабораторной работы:

1. Реализован метод опорных векторов в двойственной постановке.
2. Показано, как условия стационарности приводят к ограничениям dual-задачи.
3. Реализован трюк с ядром для построения нелинейных классификаторов.
4. Продемонстрирована роль опорных векторов в формировании решения.
5. Получены результаты, полностью совпадающие с реализацией *scikit-learn*.

Таким образом, метод SVM в двойственной форме позволяет эффективно строить как линейные, так и нелинейные классификаторы с высокой точностью.
