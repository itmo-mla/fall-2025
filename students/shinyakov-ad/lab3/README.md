# Лабораторная работа №3. SVM

В рамках лабораторной работы реализован метод опорных векторов (SVM) в двойственной постановке
с использованием трюка с ядром и проведено сравнение с эталонной реализацией `sklearn.svm.SVC`.

В качестве датасета выбран набор данных
[`Employee Future Prediction`](https://www.kaggle.com/datasets/tejashvi14/employee-future-prediction)
с задачей бинарной классификации ухода сотрудника из компании (`LeaveOrNot`).

## Структура проекта

- `src/data_load/data_load.py` — загрузка датасета с Kaggle, кодирование категориальных признаков,
  разбиение на обучающую и тестовую выборки;
- `src/model/kernels.py` — реализации ядер:
  - `LinearKernel`;
  - `PolynomialKernel` (полиномиальное ядро);
  - `RBFKernel` (гауссовское ядро);
- `src/model/svm.py` — реализация SVM через решение двойственной задачи по \(\lambda\)
  с возможностью подстановки различных ядер;
- `src/module/visualization.py` — функции для визуализации:
  - исследование данных (pairplot, распределение целевой переменной);
  - сравнение качества реализованного SVM и `sklearn.SVC`;
- `src/main.py` — основной сценарий:
  - загрузка и подготовка данных;
  - EDA и сохранение графиков;
  - обучение SVM с разными ядрами и сравнение с эталонной реализацией;
  - вывод сводной таблицы с качеством моделей.

Все графики сохраняются в директорию `artifacts/`:

- `pairplot_leaveornot.png` — попарные распределения признаков, раскрашенные по `LeaveOrNot`;
- `target_distribution.png` — распределение целевой переменной;
- `accuracy_comparison.png` — сравнение accuracy для различных ядер.

## Теоретическая часть (кратко)

1. Рассматривается геометрическая постановка задачи SVM с максимизацией зазора
   между классами и ограничениями на ошибки классификации.
2. Переход к двойственной задаче приводит к квадратичной задаче оптимизации по вектору множителей Лагранжа \(\lambda\).
3. В двойственной форме решение выражается только через скалярные произведения объектов,
   что позволяет применить трюк с ядром: заменить \(\langle x_i, x_j \rangle\) на \(K(x_i, x_j)\).
4. Рассмотрены несколько типов ядер:
   - линейное ядро;
   - полиномиальное ядро;
   - RBF‑ядро.

## Эксперименты

Для каждого из трёх ядер обучались:

- реализованный SVM в двойственной форме;
- эталонная модель `sklearn.svm.SVC` с аналогичными параметрами.

Сравнение проводилось по метрике **accuracy** на тестовой выборке.

Пример результатов (значения могут немного отличаться при повторном запуске):

| Ядро   | Custom SVM accuracy | sklearn SVC accuracy |
|--------|---------------------|----------------------|
| linear | 0.71                | 0.71                 |
| poly   | 0.66                | 0.66                 |
| rbf    | 0.77                | 0.77                 |

RBF‑ядро показывает наилучшее качество на выбранном датасете, что подтверждает пользу
использования нелинейных ядер при наличии сложной зависимости между признаками и целевой переменной.

## Запуск

Из директории `lab3`:

```bash
python main.py
```

Скрипт:

- загрузит и подготовит данные;
- построит и сохранит EDA‑графики;
- обучит SVM с линейным, полиномиальным и RBF‑ядрами;
- обучит эталонные модели `sklearn.SVC`;
- выведет сравнение accuracy и сохранит график `accuracy_comparison.png`.


