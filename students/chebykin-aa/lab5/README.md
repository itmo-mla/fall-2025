# Лабораторная работа №5

Работу выполнил студент группы Р4155 Чебыкин Артём

## 1. Предобработка данных

В работе используется **Diabetes dataset** из библиотеки `sklearn`. Датасет содержит числовые признаки и применяется в задачах линейной регрессии и анализа размерности. Пропущенные значения в данных отсутствуют.

Перед применением метода главных компонент признаки были стандартизированы по столбцам: из каждого признака вычиталось среднее значение и выполнялось деление на стандартное отклонение. Такая нормализация необходима, поскольку PCA чувствителен к масштабу признаков, а вклад главных компонент определяется дисперсией исходных данных.

## 2. Реализация OwnPCA

Метод главных компонент реализован самостоятельно на основе сингулярного разложения матрицы центрированных данных. Ниже приведён метод ```fit```, реализующий обучение модели PCA.

```python
class OwnPCA:
    def fit(self, X):
        # Центрирование данных
        self.mean_ = X.mean(axis=0)
        X_centered = X - self.mean_

        # SVD
        U, D, Vt = np.linalg.svd(X_centered, full_matrices=False)
        self.components_ = Vt
        self.singular_values_ = D

        # Объяснённая дисперсия
        self.explained_variance_ = (D ** 2) / (X.shape[0] - 1)
        self.explained_variance_ratio_ = self.explained_variance_ / self.explained_variance_.sum()

        # Определение эффективной размерности
        cumulative = np.cumsum(self.explained_variance_ratio_)
        self.n_components_eff_ = np.searchsorted(cumulative, self.variance_threshold) + 1

        return self
```

В результате работы метода вычисляются главные направления (компоненты), сингулярные значения, объяснённая дисперсия и доля объяснённой дисперсии для каждой компоненты, а также эффективная размерность выборки по заданному порогу накопленной дисперсии.

## 3. Определение эффективной размерности

Эффективная размерность определяется как минимальное число главных компонент, суммарный вклад которых превышает 95% общей дисперсии данных. Для наглядного анализа были построены графики:

* вклад каждой отдельной главной компоненты;
* накопленная объяснённая дисперсия;
* вертикальная линия, соответствующая эффективной размерности.

## 4. Сравнение с реализацией PCA из sklearn

Для проверки корректности реализации OwnPCA было проведено сравнение с эталонной реализацией `sklearn.decomposition.PCA`.

### 4.1 Сравнение графиков объяснённой дисперсии

Ниже приведено визуальное сравнение графиков объяснённой дисперсии для собственной реализации и реализации из sklearn.

| OwnPCA                                  | Sklearn PCA                                 |
| :-------------------------------------: | :-------------------------------------: |
| ![](./source/results/OwnPCA_effective_dim.png) | ![](./source/results/SklearnPCA_effective_dim.png) |

Графики демонстрируют совпадение формы кривых и одинаковое значение эффективной размерности, что подтверждает корректность реализованного алгоритма.

### 4.2 Сравнение проекций объектов

Дополнительно были выбраны несколько случайных объектов исходной выборки, для которых сравнивались координаты в пространстве главных компонент, полученные с помощью OwnPCA и sklearn PCA.

Пример сравнения для одного объекта:

```
OwnPCA: 
[-2.14398986  3.32093986 -0.6180396  -0.29043333 -0.65003407  0.39422948 0.03466934  0.12988343]
SklearnPCA: 
[ 2.14398986  3.32093986 -0.6180396   0.29043333  0.65003407 -0.39422948 0.03466934  0.12988343]
```

Совпадение значений по модулю при различии знаков является нормальным и ожидаемым результатом. Главная компонента в PCA задаёт направление в пространстве признаков, однако ориентация этого направления не фиксирована. Вектор компоненты и вектор с противоположным знаком описывают одно и то же направление.По этой причине разные реализации PCA могут возвращать компоненты, отличающиеся знаком, при этом геометрический смысл проекций и объяснённая дисперсия остаются полностью эквивалентными.
