# Лабораторная работа №5

Работу выполнил студент группы Р4155 Чебыкин Артём

## 1. Первоначальный анализ данных

В качестве датасета для решения задачи бинарной классификации был выбран **Breast Cancer Wisconsin dataset**, встроенный в библиотеку `sklearn`. Датасет состоит из числовых признаков, полученных на основе изображений опухолей молочной железы, и бинарной целевой переменной, указывающей на тип опухоли (доброкачественная или злокачественная).

Датасет не содержит пропусков и выраженных выбросов, однако признаки имеют разные масштабы. Поскольку используемые методы оптимизации (метод Ньютона–Рафсона и IRLS) чувствительны к масштабу признаков и обусловленности матриц, признаки были нормализованы по столбцам с помощью `MinMaxScaler`. Это повышает численную устойчивость алгоритмов и улучшает сходимость.

Распределение объектов по классам имеет следующий вид:

```
Распределение по классам:
1    357
0    212
Name: count, dtype: int64
```

Датасет является умеренно несбалансированным, однако степень дисбаланса не критична и не требует применения специальных методов балансировки классов.

## 2. Реализация OwnLogisticRegressionNewtonRafson

Ниже приведена реализация логистической регрессии, обучаемой с помощью **метода Ньютона–Рафсона**. В качестве целевой функции используется логистическая функция правдоподобия для бинарной классификации с метками ( y \in {0, 1} ).

На каждой итерации алгоритма:

* вычисляются вероятности принадлежности к положительному классу;
* считается градиент логарифмической функции правдоподобия;
* строится гессиан;
* решается система линейных уравнений для получения шага Ньютона;
* параметры модели обновляются.

Для повышения устойчивости используется **демпфирование гессиана**, позволяющее контролировать длину шага оптимизации и предотвращать вырождение матрицы.

```python
class OwnLogisticRegressionNewtonRafson:
    def fit(self, X: np.ndarray, y: np.ndarray):
        # Получаем размерность данных
        n_samples, n_features = X.shape

        # Начальная инициализация
        X_ext = np.hstack([X, np.ones((n_samples, 1))])
        theta = np.zeros(n_features + 1)

        for iteration in range(self.max_iter):
            # Получаем линейную комбинацию признаков
            linear = X_ext @ theta

            # Получаем вероятности классов
            probs = sigmoid(linear)

            # Диагональная матрица весов W
            W = np.diag(probs * (1 - probs))

            # Считаем градиент
            gradient = X_ext.T @ (y - probs)

            # Считаем гессиан
            hessian = X_ext.T @ W @ X_ext

            # Делаем демпфирование (контролируем шаг оптимизации)
            hessian += self.reg * np.eye(n_features + 1)

            # Решаем систему уравнений
            try:
                delta = np.linalg.solve(hessian, gradient)
            except np.linalg.LinAlgError:
                logging.info("Гессиан вырожден, обучение остановлено")
                break

            # Обновление параметров модели
            theta += delta

            if np.linalg.norm(delta) < self.tol:
                logging.info(f"сходимость на итерации {iteration}")
                break

        self.weights = theta[:-1]
        self.bias = theta[-1]

        return self
```

## 3. Реализация OwnLogisticRegressionIRLS

В данном разделе представлена реализация логистической регрессии на основе **margin-based IRLS (Iteratively Reweighted Least Squares)**. В отличие от метода Ньютона–Рафсона, данный алгоритм работает с метками классов ( y \in {-1, +1} ) и оптимизирует логистическую функцию потерь, заданную на марже ( y \cdot w^T x ).

Алгоритм IRLS последовательно решает взвешенную задачу наименьших квадратов, где веса и целевая переменная обновляются на каждой итерации в зависимости от текущих значений маржи. Для предотвращения численных проблем значения сигмоидной функции ограничиваются с помощью отсечения (`clipping`).

```python
class OwnLogisticRegressionIRLS:
    def fit(self, X: np.ndarray, y: np.ndarray):
        # Получаем размерность данных
        n_samples, _ = X.shape

        # Начальная инициализация
        F = np.hstack([X, np.ones((n_samples, 1))])
        w = np.linalg.pinv(F.T @ F) @ F.T @ y

        for iteration in range(self.max_iter):
            # Считаем сигма i
            sigma = sigmoid(y * (F @ w))
            sigma = np.clip(sigma, self.eps, 1 - self.eps)

            # Считаем гамма i
            gamma = np.sqrt(sigma * (1 - sigma))

            # Считаем F тильда
            Gamma = np.diag(gamma)
            F_tilde = Gamma @ F

            # Считаем y тильда
            y_tilde = y * np.sqrt((1 - sigma) / sigma)

            # Выбираем градиентный шаг
            try:
                delta = np.linalg.solve(
                    F_tilde.T @ F_tilde,
                    F_tilde.T @ y_tilde
                )
            except np.linalg.LinAlgError:
                logging.info("ошибка МНК")
                break

            w_new = w + delta

            if np.linalg.norm(w_new - w) < self.tol:
                logging.info(f"сходимость на итерации {iteration}")
                break

            w = w_new

        self.weights = w[:-1]
        self.bias = w[-1]
        return self
```

## 4. Сравнение с реализацией LogisticRegression из sklearn

Для сравнения качества реализованных моделей была использована стандартная реализация логистической регрессии из библиотеки `sklearn`. В качестве солвера выбран `lbfgs`, который является квазиньютоновским методом и оптимизирует ту же логистическую функцию потерь.

```python
model_sklearn = LogisticRegression(
    penalty = None,
    solver = "lbfgs",
    fit_intercept = True,
    max_iter = MAX_ITER,
    tol = TOL
)
```

Оценка качества моделей производилась с использованием метрик Accuracy, Precision, Recall и F1-score. Для корректного вычисления метрик все предсказания и истинные метки были приведены к формату `{0, 1}`.

Результаты представлены в таблице:

<div align="center">

| model                             | Accuracy | Precision | Recall  | F1 Score |
| --------------------------------- | -------- | --------- | ------- | -------- |
| OwnLogisticRegressionNewtonRafson | 0.97368  | 0.98571   | 0.97183 | 0.97872  |
| OwnLogisticRegressionIRLS         | 0.93860  | 0.98485   | 0.91549 | 0.94891  |
| LogisticRegression                | 0.94737  | 0.98507   | 0.92958 | 0.95652  |

</div>

Полученные результаты показывают, что реализация логистической регрессии с использованием метода Ньютона–Рафсона демонстрирует качество, сопоставимое и даже превосходящее реализацию из `sklearn`. Margin-based IRLS показывает более низкие значения метрик, что объясняется аппроксимационным характером метода и его меньшей численной устойчивостью по сравнению с методами второго порядка.