# k-NN с методом Парзеновского окна и отбором эталонов (CCV)

Проект реализует алгоритм k-ближайших соседей (k-NN) с использованием метода Парзеновского окна переменной ширины, а также алгоритм отбора эталонов на основе минимизации критерия скользящего контроля (CCV - Complete Cross-Validation).

## Теоретическая справка

### Метод Парзеновского окна с переменной шириной

Ширина окна $h$ выбирается переменной для каждого объекта и равна расстоянию до $k$-го ближайшего соседа:
$$ h(u) = \rho(u, x_u^{(k)}) $$

Вес $i$-го соседа вычисляется как:
$$ w_i = K\left(\frac{\rho(u, x_i)}{h(u)}\right) $$

В проекте реализованы ядра: Гауссово, Треугольное, Епанечникова.

### Отбор эталонов (CCV)
CCV (**Complete Cross-Validation**) — вероятность правильной классификации объектов при использовании текущего набора прототипов.

Формула вычисления CCV:

$$ CCV(\Omega) = \frac{1}{L} \sum_{i=1}^L \sum_{m=1}^k \left[ y_i \neq y_i^{(m|\Omega)} \right] \frac{C_{L-1-m}^{\ell-1}}{C_{L-1}^{\ell-1}} $$

Где $T(x_i, \Omega)$ — вклад объекта $x_i$ в CCV.


## Архитектура проекта

Проект организован в виде модулей:

*   **`source/knn.py`**: Реализация метрического классификатора.

*   **`source/PrototypeSelector.py`**: Модуль для отбора эталонных объектов.

*   **`source/utils.py`**: Вспомогательные функции, включая реализации ядер сглаживания, функции кросс-валидации (LOO) и визуализации результатов.

*   **`main_notebook.ipynb`**: Основной сценарий выполнения работы, включающий загрузку данных, обучение моделей, оптимизацию гиперпараметров и сравнительный анализ результатов.

## Результаты работы

Результаты получены на датасете **Breast Cancer Wisconsin**.

1.  **Оптимизация k**:
    *   Методом Leave-One-Out (LOO) найдено оптимальное значение $k=10$ с ошибкой ~0.0302.

2.  **Отбор эталонов**:
    *   Алгоритм успешно сократил обучающую выборку с ~398 объектов до ~64 эталонов (примерно в 6 раз).
    *   График CCV демонстрирует характерное поведение: падение ошибки на начальных итерациях (удаление шума), стабилизация, и резкий рост в конце (когда начинают удаляться важные эталоны).

3.  **Сравнение классификаторов**:

    | Метрика | Prototype KNN (на эталонах) | Custom KNN (на всех данных) | Sklearn KNN |
    | :--- | :--- | :--- | :--- |
    | **Accuracy** | **0.9591** | 0.9708 | 0.9708 |
    | **Precision**| **0.9903** | 0.9725 | 0.9725 |
    | **Recall** | 0.9444 | 0.9815 | 0.9815 |
    | **F1-score** | 0.9668 | 0.9770 | 0.9770 |

    **Вывод**: Использование сокращенного набора эталонов (Prototype KNN) привело к незначительному снижению Accuracy (~1.2%) по сравнению с использованием полной выборки, при этом к значительному сокращению выборки. Интересно отметить, что точность (Precision) у метода на эталонах оказалась даже выше, чем у классификатора на полных данных, что говорит об успешном удалении шумовых граничных объектов. Также алгоритм показал идентичные значения метрик по сравнению с эталонной реализацией.
