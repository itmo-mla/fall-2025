# Лабораторная работа 4: PCA через сингулярное разложение

## Теоретические основы

### Метод главных компонент (PCA)

Метод главных компонент (Principal Component Analysis, PCA) — это метод снижения размерности данных, который позволяет заменить исходные признаки меньшим числом новых признаков, сохраняя при этом основную информацию о данных. PCA находит направления максимальной дисперсии в данных и проецирует данные на эти направления.

---

### Связь PCA с сингулярным разложением

PCA можно эффективно реализовать через сингулярное разложение (SVD) матрицы данных.

Рассмотрим центрированную матрицу данных X размерности n × p, где:
- n — количество объектов (образцов)
- p — количество признаков

Для такой матрицы выполняется сингулярное разложение:

X = U Σ Vᵀ

где:
- U — матрица левых сингулярных векторов размерности n × p
- Σ — диагональная матрица сингулярных значений размерности p × p
- Vᵀ — матрица правых сингулярных векторов размерности p × p

Строки матрицы Vᵀ соответствуют главным компонентам.

Сингулярные значения связаны с собственными значениями ковариационной матрицы следующим образом:

λ_i = σ_i² / (n − 1)

Здесь σ_i — сингулярные значения, а λ_i — собственные значения ковариационной матрицы, которые представляют собой объяснённую дисперсию.

---

### Эффективная размерность выборки

Эффективная размерность выборки — это наименьшее целое число m, при котором относительная ошибка аппроксимации данных не превышает заданный порог ε.

Относительная ошибка аппроксимации определяется формулой:

E_m = (λ_{m+1} + ... + λ_n) / (λ_1 + ... + λ_n)

где:
- λ_i — собственные значения ковариационной матрицы, упорядоченные по убыванию
- ε — пороговое значение (обычно 0.01 или 0.05)
- E_m — относительная ошибка при использовании m главных компонент

Для выбора числа главных компонент также применяется критерий крутого склона, который позволяет визуально определить момент, начиная с которого вклад последующих компонент становится незначительным.

---

## Особенности реализации

### Структура проекта

Проект имеет следующую структуру:

- source/pca_svd.py — реализация класса PCA_SVD для выполнения PCA через SVD
- source/visualization.py — функции для визуализации результатов
- source/comparison.py — функции для сравнения с эталонной реализацией из библиотеки sklearn
- main.ipynb — ноутбук с демонстрацией работы алгоритма

---

### Класс PCA_SVD

Класс PCA_SVD реализует метод главных компонент с использованием сингулярного разложения и включает следующие этапы:

1. Центрирование данных — вычитание среднего значения по каждому признаку.
2. SVD-разложение — применение функции np.linalg.svd к центрированной матрице данных.
3. Вычисление метрик:
   - объяснённая дисперсия: λ_i = σ_i² / (n − 1);
   - отношение объяснённой дисперсии: λ_i / Σ λ_j.
4. Проецирование данных — преобразование исходных данных в пространство главных компонент.

---

## Результаты

### Используемый датасет

В работе используется датасет California Housing из библиотеки sklearn.

Характеристики датасета:
- количество объектов: 20640;
- количество признаков: 8;
- признаки включают медианный доход, медианный возраст домов, среднее количество комнат и другие.

---

### Эффективная размерность

При пороговом значении ε = 0.01 эффективная размерность выборки составляет m = 3.

Это означает, что первые шесть главных компонент сохраняют более 99% общей дисперсии данных. Оставшиеся две компоненты содержат менее 1% информации и могут рассматриваться как шум.

---

### Сравнение с реализацией sklearn

Реализация PCA через SVD показала полную эквивалентность с эталонной реализацией sklearn.decomposition.PCA:

- главные компоненты совпадают;
- значения объяснённой дисперсии совпадают;
- преобразованные данные эквивалентны, различия могут быть связаны только с направлением компонент.
