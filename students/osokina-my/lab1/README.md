# Классификация и визуализация данных

## Теоретические основы

В работе используется датасет `breast_cancer` из `sklearn`, содержащий числовые признаки опухолей и бинарную метку класса. Для анализа структуры данных применяются методы понижения размерности:

- **PCA** — линейный метод, строящий ортогональные компоненты, объясняющие максимальную дисперсию.

Градиентный спуск (GD) обновляет веса по градиенту, вычисленному на всей выборке, что даёт стабильные шаги, но требует больших вычислений. Стохастический градиентный спуск (SGD) делает шаги по одному объекту (или мини‑батчу), поэтому работает быстрее на больших данных и лучше «выбирается» из локальных минимумов, но даёт более шумные обновления.

Классификация проводится двумя вариантами стохастического градиентного спуска:

- `SGDClassifier` из `sklearn` с логистической функцией потерь (log loss) и L2-регуляризацией.
- Собственная реализация SGD (`source/sgd.py`) с отслеживанием функции потерь на обучающей и тестовой выборках.

Используется L2‑регуляризация: к функции потерь добавляется штраф `alpha * ||w||^2`, а в градиент добавляется `alpha * w`.

Отступ (margin) объекта определяется как `m = y * (w^T x)`: знак показывает правильность классификации, модуль — уверенность. Для логистической функции потерь `log(1 + exp(-m))` градиент по весам равен `-x * y / (1 + exp(m))` (с добавлением регуляризационного члена для L2).

Качество обучения отслеживается через рекуррентную оценку функционала: `Q_t = forget_rate * L(m_t) + (1 - forget_rate) * Q_{t-1}` — это экспоненциально сглаженная оценка потерь. В реализации предусмотрено предъявление объектов по модулю отступа (ordering по margin): сначала идут объекты с большим |m| (уверенно классифицируемые), а в конце — с малым |m| (пограничные).

Для инициализации весов используются три режима: случайная инициализация, инициализация через корреляцию признаков с метками (`w_j = <x_j, y> / ||x_j||^2`), а также мультистарт (несколько случайных запусков с выбором лучшей по `Q` модели). Инициализация через корреляцию задаёт веса в направлении, где признак сильнее всего связан с классом, поэтому стартовое решение сразу ориентировано на разделение классов, а не на случайное направление.

Качество оценивается стандартными метриками: accuracy, precision, recall и F1-score. Дополнительно анализируются **margin**‑значения, показывающие уверенность классификатора.

## Реализация

1. Загружен датасет и преобразованы метки классов в `{-1, 1}`.
2. Построены двумерные визуализации с помощью PCA.
3. Данные разделены на обучающую и тестовую выборки, применён `MinMaxScaler`.
4. Обучена модель `SGDClassifier` с `partial_fit` в нескольких эпохах (L2‑регуляризация).
5. Реализован SGD с логистической функцией потерь.
6. Реализована рекуррентная оценка функционала качества и сохранена динамика потерь.
7. Реализовано предъявление объектов по модулю отступа и анализ margin для выделения шумовых, пограничных и надёжных точек.
8. Реализованы способы инициализации весов: случайная, через корреляцию и мультистарт; построен график train/test loss и рассчитаны метрики.

## Результаты

Сравнение метрик:

| Модель | Accuracy | Precision | Recall | F1-score |
| --- | --- | --- | --- | --- |
| SGDClassifier (sklearn) | 0.947 | 0.948 | 0.947 | 0.947 |
| Собственная SGD | 0.942 | 0.942 | 0.942 | 0.941 |

График потерь показывает стабильное уменьшение функции потерь на обучении и согласованное поведение на тесте, что указывает на корректную сходимость без выраженного переобучения.

## Выводы

- Собственная реализация SGD даёт сравнимые результаты с `sklearn`, что подтверждает корректность реализации.
- Анализ margin позволяет выделить трудные и шумовые наблюдения, что полезно для дальнейшего улучшения модели и диагностики данных.
