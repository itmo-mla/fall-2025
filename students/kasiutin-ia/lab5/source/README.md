# Лабораторная работа №5. Логистическая регрессия

## Теоретическая часть
На лекции рассмотрели многомерную нелинейную регрессию. Свели задачу многомерной нелинейной регрессии к итерационному процессу IRLS. Рассмотрели решение задачи логистической регрессии с помощью метода Ньютона-Рафсона и IRLS. С другой стороны показали, как вероятностная постановка задачи классификации приводит к логистической регрессии.

## Описание датасета
1. [Датасет](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package) агрегирует информацию об атмосферных данных с целью предсказания наличия дождя на следующий день. В нем присутствуют как количественные, так и категориальные признаки. В ходе предварительного анализа данных (initial_eda.ipynb) были оставлены количественные признаки c пропусками менее 30%, а также бинарный признак RainToday. Все признаки были нормализованы.

- `Pressure9am` - атмосферное давление в 9:00
- `Pressure3pm` - давление в 15:00
- `MaxTemp` - максимальная температура за день
- `MinTemp` - минимальная температура за день
- `Temp9am` - температура в 9:00
- `Temp3pm` - температура в 15:00
- `WindGustSpeed` - максимальная скорость ветра за день
- `WindSpeed9am` - скорость ветра в 9:00
- `WindSpeed3pm` - скорость ветра в 15:00
- `RainToday` - факт дождя сегодня (0 или 1)
- `Rainfall` - кол-во осадков за день
- `Humidity9am` - влажность в 9:00
- `Humidity3pm` - влажность в 15:00

## Реализация моделей

### Метод Ньютона-Рафсона
Реализована логистическая регрессия с использованием метода Ньютона-Рафсона для оптимизации весов модели. Ниже приведен код реализации метода:

```python
self.weights = np.linalg.inv(X.T @ X) @ X.T @ y

for i in range(n_iter):
    sigma = self._sigmoid(self.weights @ X.T * y)

    grad = - np.linalg.inv(X.T @ np.diag((1 - sigma) * sigma) @ X + np.eye(X.shape[1]) * 1e-4) @ X.T @ (y / sigma)


    self.weights -= learning_rate * grad
```

В ходе вычисления обратной матрицы методом np.linalg.inv возможна ошибка из-за обращения сингулярной матрицы. Решается это путем добавления диагональной матрицы с малыми значениями, что избавляет от возможной линейной зависимости значений.

### IRLS (Iteratively Reweighted Least Squares)
Реализована логистическая регрессия с использованием метода IRLS для оптимизации весов модели. Код метода:

```python
self.weights = np.linalg.inv(X.T @ X) @ X.T @ y

for i in range(n_iter):
    sigma = self._sigmoid(self.weights @ X.T * y)
    gamma = np.sqrt((1 - sigma) * sigma)

    F_hat = np.diag(gamma) @ X
    y_hat = y * np.sqrt((1 - sigma) / sigma)

    grad = np.linalg.inv(F_hat.T @ F_hat) @ F_hat.T @ y_hat


    self.weights += learning_rate * grad
```

### Эталонная реализация (sklearn)
Для сравнения использовалась эталонная реализация логистической регрессии из библиотеки sklearn.linear_model.LogisticRegression.

## Результаты

| Модель | Accuracy | Precision | Recall | F1-Score |
|--------|----------|-----------|--------|----------|
| Custom Newton-Raphson | 0.841 | 0.705 | 0.474 | 0.567 |
| Custom IRLS | 0.842 | 0.715 | 0.470 | 0.567 |
| sklearn LogisticRegression | 0.838 | 0.724 | 0.461 | 0.563 |

Как видно из сравнительной таблицы, все 3 модели показывают схожие результаты. Небольшое расхождение результатов метода Ньютона-Рафсона и IRLS объясняется разным числом эпох обучения и небольшой поправки в виде диагональной матрицы в методе Ньютона-Рафсона
