# Лабораторная работа №1. Линейная классификация

В рамках лабораторной работы предстоит реализовать линейный классификатор. И обучить его методом стохастического градиентного спуска с инерцией с L2 регуляризацией и квадратичной функцией потерь.

## Задание

1. выбрать датасет для классификации, например на [kaggle](https://www.kaggle.com/datasets?&tags=13304-Clustering);
2. реализовать вычисление отступа объекта (визуализировать, проанализировать);
3. реализовать вычисление градиента функции потерь;
4. реализовать рекуррентную оценку функционала качества;
5. реализовать метод стохастического градиентного спуска с инерцией;
6. реализовать L2 регуляризацию;
7. реализовать скорейший градиентный спуск;
8. реализовать предъявление объектов мо модулю отступа;
9. обучить линейный классификатор на выбранном датасете;
   1. обучить с инициализацией весов через корреляцию;
   2. обучить со случайной инициализацией весов через мультистарт;
   3. обучить со случайным предъявлением и с п.8;
10. оценить качество классификации;
11. сравнить лучшую реализацию с эталонной;
12. подготовить отчет.

### 1. Выбор датасета
Выбранный датасет: breast_cancer
```text
Размеры данных:
Обучающая выборка: (314, 30)
Валидационная выборка: (142, 30)
Тестовая выборка: (113, 30)
```

### 2. Сравнение методов
Лучшим по точности оказался steepest descent со случайной инициализацией и случайным предъявлением.
![Comparison](img\comparison_histogram.png)
Метрики:
```text
Accuracy: 0.9930
Precision: 0.9890
Recall: 1.0000
F1-Score: 0.9945
Confusion Matrix:
        Predicted
Actual      0      1
     0     51      1
     1      0     90
```
Полученные веса:
![Best weights](img\best_model_weights.png)

### 3. Обучение
![Steepest descent history](img\steepest_descent_history.png)
![Steepest descent margins](img\steepest_descent_margins.png)

### 4. Сравнение с эталоном
За эталон взят ```sklearn.linear_model.SGDClassifier```
Метрики:
```text
Accuracy: 0.9789
Precision: 0.9677
Recall: 1.0000
F1-Score: 0.9836
Confusion Matrix:
        Predicted
Actual      0      1
     0     49      3
     1      0     90
```
